\section{Case study}
\label{sec:case_study}
\subsection*{Patterns of multi-controller failure cases}

\paragraph*{How are these failures created?}
Large systems are developed by people in different teams and different organizations. One team or a certain group of people in case of open source is responsbiel for developing and maintaining one controller. It is impossible for them to know how all other controllers work in detail. It is not feasible to make a controller not incuring any conflicted cases with all other controller for all possible cases. That is not even desirable development direction because then the process will take too long.
How Kubernetes function is also contributing to this type of failure. What each controller does is periodically monitoring if the associated part in the cluster is currently in desirable status or not. If it is not, it will try to bring it back to the desirable status, running reconciliation process. For example, if the current number of pods is less than what it should be based on HPA configuration. HPA will detect it and increase the number of pods. The important part is a controller does not care what other controllers are doing and it just see the current status and run its own logic to put the cluster back to the right place.


\paragraph*{Why is it hard to prevent these failures?}
There are different categories of failures existing in Kubernetes. Inherently, it is ambiguous to classify such failure cases as bug. Each controller is working as it is supposed to do and clearly there is no bug in its logic if you look them individually. Some of answers to reported github issues was that developers in Kubernetes community maintaining the controller are aware of or admit the issue but they ended up not fixing it while saying it is their design choice or there is no clear way to patch it. It is because often the clear solution does not exist not because Kubernetes community constantly procrastinate or neglect them. Most of problems we are presenting in this paper could be prevented if you know it will happen before you apply them to your cluster. For the example in D1 failure case, if you had known that Deployment configuration and Scheduler policy are contradictorial each other, you would have not configured in such self-destroying way. However, there are several reasons that it is still not trivial to avoid this type of failures. The first reason is scalability. The nubmer of pods could be easily over a thousand and the number of services deployed in the cluster could be easily over a hundred. To configure this scale of Kubernetes cluster having different applications running, you may need to manage a large number of configuration yaml files for each deployment, nodes, hpa, schedulers, etc. In this scale and complexity, it is challenging to make all controllers always in a coordinated manner in the cluster-wise level. The second reason is that in a large scale cluster some failrues are not noticeable and slowly gnawing the cluster's resources. For example, in S1 failure case, node utilization on average could stay within some range in the half way of scheduler config and deschduler config. It is possible that the cluster resource utilization looks stable even though what is happening beind is deschduler keeping evicting pods from high utilization node and scheduler placing pods in low utilization node makig the utilization high again. The third reasons is that semantically contradictorial configurations does not mean it will always lead to failrues. Many of these problems are triggered in a specific number of nodes and pods. 

\begin{table*}[t]
    \centering
    \begin{tabular}{c c c c c c}
    \hline
        Case & Categories & Controllers & Properties & Behaviors  \\ 
        \hline
        D1(R) & Conflicted config & Deployment + Kubelet & Liveness & Scheduling and evicting pods infinitely  \\ 
        H1(R) & Lack of context & HPA + App CPU changes & Safety & HPA is agnostic to app  \\ 
        H2(R) & Conflicted config & HPA + Deployment & Safety & Sub-optimal scaling behavior  \\ 
        H3(R) & Imperfect knowledge & HPA + Node reachability & Safety & Semantically wrong avg CPU util (reachability vs healthiness)  \\ 
        S1 & Conflicted config & Scheduler + Descheduler & Liveness & High utilization(scheduler) <-> Low utilization(descheduler) \\ 
        S2(R) & Conflicted config & Scheduler + Descheduler & Liveness & Deployment preference <-> Violation in maxSkew \\ 
        S3(R) & Conflicted config & Scheduler & Liveness & Two pod spread constraints are conflicted each other  \\ 
        S5 & Conflicted config & Scheduler & Safety & Pods are scheduled to one node, because of lopsided preference  \\ 
        S6(R) & Lack of feature & Scheduler & Safety & Scheduler is not able to adjust skewed placement  \\ 
        S7(R) & Lack of context & Scheduler + Kubelet & Liveness & Scheduler includes NotReady node for maxSkew calculation  \\ 
        \hline
    \end{tabular}
    \caption{}
    \label{table:summary}
\end{table*}

\subsection*{Detailed examples}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.4\textwidth]{figure/num_pod.pdf}
    \caption{Number of pods over time for four failure cases.}
    \label{fig:num_pod}
\end{figure}


\subsection*{Observation 1}
